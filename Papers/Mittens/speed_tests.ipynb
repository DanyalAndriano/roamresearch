{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe implementation speed tests (section 2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'Nick Dingwall and Christopher Potts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires a compiled version of [the official GloVe code release](https://nlp.stanford.edu/projects/glove/) to be in the directory `official-glove/build`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import timeit\n",
    "import random\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "from utils import build_weighted_matrix\n",
    "\n",
    "# GloVe implementations:\n",
    "from nonvectorized_glove import GloVeModel\n",
    "from tf_mittens import Mittens\n",
    "from vector_glove import VectorGlove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random corpora\n",
    "\n",
    "We build simulated corpora from a Zipfian distribution of words, and we build matrices from them using the same methods used for our empirical corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_corpus(n_words=1000000):\n",
    "    \"\"\"Returns a string of integers with a Zipfian distribution.\"\"\"\n",
    "    # For `n_words` at 1000 or more, these settings tend \n",
    "    # to return matrices that about the sparsity we see in\n",
    "    # our empirical matrices.\n",
    "    words = [str(i) for i in np.random.zipf(1.7, n_words)]\n",
    "    corpus = \" \".join(words)\n",
    "    return corpus     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_corpus_and_matrix(vocab_size, window_size):\n",
    "    \"\"\"Creates a corpus and associated matrix. This helps\n",
    "    ensure parity between tests with the official distribution,\n",
    "    where we start by reading in a corpus, and tests with the\n",
    "    Python implementations, where we start with a matrix.    \n",
    "    \"\"\"\n",
    "    # Setting `n_words` this way is an attempt to ensure\n",
    "    # that we get a matrix of the right size we want. For\n",
    "    # large vocabularies, really big corpora are needed.\n",
    "    n_words = np.min([vocab_size * 10000, int(5e8)])\n",
    "    corpus = generate_corpus(n_words)    \n",
    "    tokenizer = lambda x: x.split(' ')\n",
    "    X = build_weighted_matrix(\n",
    "        [corpus], \n",
    "        tokenizing_func=tokenizer, \n",
    "        vocab_size=vocab_size, \n",
    "        window_size=window_size)\n",
    "    return X.values, corpus        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 50\n",
    "xmax = 100\n",
    "alpha = 0.75\n",
    "max_iter = 10\n",
    "eta = 0.01\n",
    "tol = 1e-4\n",
    "window_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Official GloVe distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def official_glove_experiment(corpus, vocab_size, verbose=False):\n",
    "    BUILDDIR = 'official-glove/build'    \n",
    "    CORPUS_FILE = 'official-glove/speed-test-corpus.txt'\n",
    "    VOCAB_FILE = 'official-glove/vocab.txt'\n",
    "    COOCCUR_FILE = 'official-glove/cooccurrence.bin'\n",
    "    SHUFFLE_FILE = 'official-glove/cooccurrence.shuf.bin'\n",
    "    VECTORS_FILE = 'official-glove/vectors'\n",
    "    \n",
    "    with open(CORPUS_FILE, 'wt') as f:\n",
    "        f.write(corpus)                \n",
    "    \n",
    "    VERBOSE = 0\n",
    "    MEMORY = 4.0\n",
    "    NUM_THREADS = 1\n",
    "        \n",
    "    vocab_cmd = [\n",
    "        '{}/vocab_count'.format(BUILDDIR),\n",
    "        '-max-vocab', str(vocab_size),\n",
    "        '-min-count', '1',\n",
    "        '< {} > {}'.format(CORPUS_FILE, VOCAB_FILE)]    \n",
    "    \n",
    "    cooccur_cmd = [\n",
    "        '{}/cooccur'.format(BUILDDIR), \n",
    "        '-memory', str(MEMORY),\n",
    "        '-verbose', '0',\n",
    "        '-vocab-file', VOCAB_FILE,\n",
    "        '-window-size', str(window_size),\n",
    "        '< {} > {}'.format(CORPUS_FILE, COOCCUR_FILE)]  \n",
    "    \n",
    "    shuffle_cmd = [\n",
    "        '{}/shuffle'.format(BUILDDIR),\n",
    "        '-memory', str(MEMORY), \n",
    "        '-verbose', '0',\n",
    "        '< {} > {}'.format(COOCCUR_FILE, SHUFFLE_FILE)]\n",
    "    \n",
    "    glove_cmd = [\n",
    "        '{}/glove'.format(BUILDDIR), \n",
    "        '-save-file', VECTORS_FILE, \n",
    "        '-threads', str(NUM_THREADS), \n",
    "        '-input-file', SHUFFLE_FILE,\n",
    "        '-x-max', str(xmax),\n",
    "        '-iter', str(max_iter),\n",
    "        '-vector-size', str(n),\n",
    "        '-binary', '0',\n",
    "        '-vocab-file', VOCAB_FILE]\n",
    "    \n",
    "    for cmd in [vocab_cmd, cooccur_cmd, shuffle_cmd]:        \n",
    "        x = subprocess.run(\" \".join(cmd), shell=True, check=True, stdout=subprocess.PIPE)\n",
    "        if verbose:\n",
    "            print(\"=\"*70)\n",
    "            print(\" \".join(cmd) + \";\")\n",
    "            print(x)\n",
    "        \n",
    "    def run_test():\n",
    "         subprocess.run(glove_cmd)\n",
    "                        \n",
    "    secs = timeit.timeit(run_test, number=1)\n",
    "    \n",
    "    VECTORS_FILE = VECTORS_FILE + \".txt\"\n",
    "            \n",
    "    X = pd.read_csv(VECTORS_FILE, delim_whitespace=True, index_col=0).values\n",
    "      \n",
    "    for f in [CORPUS_FILE, VOCAB_FILE, COOCCUR_FILE, SHUFFLE_FILE, VECTORS_FILE]:\n",
    "        os.remove(f)\n",
    "                \n",
    "    return secs, X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorized_tensorflow_experiment(X):\n",
    "    model = Mittens(\n",
    "        n=n, \n",
    "        xmax=xmax, \n",
    "        alpha=alpha, \n",
    "        max_iter=max_iter, \n",
    "        eta=eta, \n",
    "        tol=tol,\n",
    "        display_progress=0)\n",
    "    \n",
    "    def run_test():    \n",
    "        model.fit(X)\n",
    "\n",
    "    return timeit.timeit(run_test, number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-vectorized Tensorflow\n",
    "\n",
    "Adapted Grady Simon (https://github.com/GradySimon/tensorflow-glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nonvectorized_tensorflow_experiment(X):    \n",
    "    model = GloVeModel(\n",
    "        n=n, \n",
    "        alpha=alpha, \n",
    "        xmax=xmax,\n",
    "        eta=eta,\n",
    "        max_iter=max_iter)\n",
    "    \n",
    "    def run_test():\n",
    "        model.fit(X)\n",
    "        \n",
    "    return timeit.timeit(run_test, number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorized_numpy_experiment(X):\n",
    "    model = VectorGlove(\n",
    "        n=n, \n",
    "        xmax=xmax, \n",
    "        alpha=alpha, \n",
    "        max_iter=max_iter,\n",
    "        learning_rate=eta, \n",
    "        display_progress=False)\n",
    "    \n",
    "    def run_test():\n",
    "        model.fit(X)\n",
    "        \n",
    "    return timeit.timeit(run_test, number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def timing_experiment(\n",
    "        n_tests=5, \n",
    "        vocab_sizes=(5000, 10000, 20000),\n",
    "        funcs=(vectorized_numpy_experiment,\n",
    "               nonvectorized_tensorflow_experiment,\n",
    "               vectorized_tensorflow_experiment,\n",
    "               official_glove_experiment)):               \n",
    "    data = []            \n",
    "    for vocab_size in vocab_sizes:   \n",
    "        print(\"Vocab size: {:,}\".format(vocab_size))\n",
    "        for t in range(1, n_tests+1):\n",
    "            X, corpus = generate_corpus_and_matrix(vocab_size, window_size) \n",
    "            print(\"\\tX has vocab size {:,} and {:,} non-0 entries\".format(\n",
    "                    X.shape[0], np.count_nonzero(X)))\n",
    "            for func in funcs:\n",
    "                print(\"\\t\", t, func.__name__)                \n",
    "                if func.__name__ == 'official_glove_experiment':\n",
    "                    secs, X = func(corpus, vocab_size)\n",
    "                else:                \n",
    "                    secs = func(X)\n",
    "                experiment_name = func.__name__.replace(\"_experiment\", \"\")\n",
    "                data.append({\n",
    "                    'test_num': t, \n",
    "                    'iterations': max_iter,\n",
    "                    'vocab_size': X.shape[0],\n",
    "                    'model': experiment_name, \n",
    "                    'seconds': secs})\n",
    "                \n",
    "        pd.DataFrame(data).to_csv(\"tmp-speed-interim_to{}.csv\".format(vocab_size))\n",
    "\n",
    "    df = pd.DataFrame(data)     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize(data, digits=2):\n",
    "    results = data.groupby(['model', 'vocab_size']).apply(\n",
    "        lambda x: x['seconds'].sum() / x['iterations'].sum())\n",
    "    results = results.to_frame().rename(columns={0: 'mean seconds per iteration'})\n",
    "    return results.round(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpu_data = timing_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpu_data_20K = timing_experiment(\n",
    "     funcs=(vectorized_numpy_experiment,\n",
    "               nonvectorized_tensorflow_experiment,\n",
    "               vectorized_tensorflow_experiment,\n",
    "               official_glove_experiment),\n",
    "    vocab_sizes=(20000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpu_data.to_csv(\"results/speed-tests-cpu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summarize(cpu_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_data = timing_experiment(\n",
    "     funcs=(vectorized_tensorflow_experiment,\n",
    "            nonvectorized_tensorflow_experiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_data.to_csv(\"results/speed-tests-gpu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summarize(gpu_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
